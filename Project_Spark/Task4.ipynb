{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbe62928-01d3-402d-9e45-19d009a4639d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Names of people in the group\n",
    "\n",
    "Please write the names of the people in your group in the next cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15e1606f-bbea-4e98-9f90-bf0a15da5391",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Name of person A: Dominik Ucher\n",
    "\n",
    "Name of person B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "389ff132-7e3c-444e-a980-6490e3448153",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Loading modules that we need\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Add your imports below this line\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import avg, stddev_pop\n",
    "\n",
    "spark = SparkSession.builder.appName(\"task4\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b384d61-f63a-4c3f-9d87-63e0427c5ecd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# A helper function to load a table (stored in Parquet format) from DBFS as a Spark DataFrame \n",
    "def load_df(table_name: \"name of the table to load\") -> DataFrame:\n",
    "    return spark.read.parquet(table_name)\n",
    "\n",
    "users_df = load_df(\"/user/hive/warehouse/users.parquet\")\n",
    "posts_df = load_df(\"/user/hive/warehouse/posts.parquet\")\n",
    "\n",
    "# Uncomment if you need\n",
    "comments_df = load_df(\"/user/hive/warehouse/comments.parquet\")\n",
    "badges_df = load_df(\"/user/hive/warehouse/badges.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9b9e457-0510-45fb-8a13-85c006247f0c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### The Problem: Mining the Interests of Experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d47852ba-d784-407b-988b-aa3efc5039f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# FUNKSJONER\n",
    "\n",
    "def compute_pearsons_r(df: DataFrame, col1: str, col2: str) -> float:\n",
    "    df.createOrReplaceTempView(\"df\")\n",
    "    result = spark.sql(f\"SELECT (AVG({col1} * {col2}) - (AVG({col1}) * AVG({col2}))) / (stddev_pop({col1}) * stddev_pop({col2})) pearsons_r FROM df\")\n",
    "\n",
    "    pearsons_r = result.collect()[0]['pearsons_r']\n",
    "\n",
    "    return pearsons_r\n",
    "\n",
    "def make_tag_graph(df: DataFrame) -> DataFrame:\n",
    "    df.createOrReplaceTempView(\"df\")\n",
    "    data = []  # Helper dataset\n",
    "    tagsCollect = df.collect()\n",
    "\n",
    "    for row in tagsCollect:\n",
    "        tags = row['Tags']\n",
    "        \n",
    "        if tags:  \n",
    "            # Ensure tags is a list before processing\n",
    "            if not isinstance(tags, list):\n",
    "                tags = [tags]\n",
    "            \n",
    "            for tag in tags:\n",
    "                tagdata = tag.split(\">\")\n",
    "                tagdata = tagdata[:-1]\n",
    "                if (len(tagdata) == 1):\n",
    "                    tag = tagdata[0].replace(\"<\", \"\").strip()\n",
    "                    data.append((tag,tag))\n",
    "                else:\n",
    "                    for i in range(len(tagdata) - 1):\n",
    "                        tagdata[i] = tagdata[i].replace(\"<\", \"\")\n",
    "                        u = tagdata[i]\n",
    "                        for j in range(i+1, len(tagdata)):\n",
    "                            v = tagdata[j].replace(\"<\", \"\").strip()\n",
    "                            data.append((u, v))\n",
    "                            data.append((v, u))\n",
    "    \n",
    "    if len(data) == 0:\n",
    "        return None\n",
    "    \n",
    "    tag_graph = spark.createDataFrame(data, [\"u\", \"v\"])\n",
    "    return tag_graph\n",
    "\n",
    "\n",
    "def make_tag_graph2(df: DataFrame) -> DataFrame:\n",
    "    df.createOrReplaceTempView(\"df\")\n",
    "    data = []  # Helper dataset\n",
    "    tagsCollect = df.collect()\n",
    "\n",
    "    for row in tagsCollect:\n",
    "        tags = row['Tags']\n",
    "        \n",
    "        if tags:  \n",
    "            tagdata = tags.split(\">\")\n",
    "            tagdata = tagdata[:-1]\n",
    "            if (len(tagdata) == 1):\n",
    "                tag = tagdata[0].replace(\"<\", \"\").strip()\n",
    "                data.append((tag,tag))\n",
    "            else:\n",
    "                for i in range(len(tagdata) - 1):\n",
    "                    tagdata[i] = tagdata[i].replace(\"<\", \"\")\n",
    "                    u = tagdata[i]\n",
    "                    for j in range(i+1, len(tagdata)):\n",
    "                        v = tagdata[j].replace(\"<\", \"\").strip()\n",
    "                        data.append((u, v))\n",
    "                        data.append((v, u))\n",
    "    if len(data) == 0:\n",
    "        return None\n",
    "    \n",
    "    tag_graph = spark.createDataFrame(data, [\"u\", \"v\"])\n",
    "    return tag_graph\n",
    "\n",
    "\n",
    "def get_nodes(df: DataFrame) -> DataFrame:\n",
    "    df.createOrReplaceTempView(\"df\")\n",
    "\n",
    "    tags = spark.sql(\"SELECT DISTINCT u as id FROM df WHERE u IS NOT NULL AND u != '' UNION SELECT DISTINCT v as id FROM df WHERE v IS NOT NULL AND v != ''\")\n",
    "\n",
    "    tag_graph = tags.select(\"id\").distinct()\n",
    "\n",
    "    return tag_graph\n",
    "\n",
    "\n",
    "def get_edges(df: DataFrame) -> DataFrame:\n",
    "    df.createOrReplaceTempView(\"df\")\n",
    "    data = []  \n",
    "    tagsCollect = df.collect()\n",
    "    \n",
    "    for row in tagsCollect:\n",
    "        u = row['u']\n",
    "        v = row['v']\n",
    "\n",
    "        if u != v:\n",
    "            data.append((u, v))       \n",
    "    if len(data) > 0:\n",
    "        edge_graph = spark.createDataFrame(data, [\"src\", \"dst\"])\n",
    "        return edge_graph\n",
    "    else:\n",
    "        return None \n",
    "       \n",
    "    return edge_graph\n",
    "  \n",
    "\n",
    "def variableA(df: DataFrame, user: int) -> float:\n",
    "    df.createOrReplaceTempView(\"df\")\n",
    "    result = spark.sql(f\"SELECT CAST(Reputation AS INT) AS Reputation FROM df WHERE Id={user}\")\n",
    "    return result.collect()[0]['Reputation']\n",
    "\n",
    "\n",
    "def variableB(df: DataFrame, user: int) -> float:\n",
    "  df.createOrReplaceTempView(\"df\")\n",
    "  questions = spark.sql(f\"SELECT * FROM df WHERE OwnerUserId = {user}\")\n",
    "  tag_graph = make_tag_graph(questions)\n",
    "  if (tag_graph == None):\n",
    "    return 0\n",
    "  # nodes = get_nodes(tag_graph).count()\n",
    "  nodes = 638\n",
    "  edgegraph = get_edges(tag_graph)\n",
    "  if edgegraph is None:\n",
    "    return 0\n",
    "  else:\n",
    "    edges = edgegraph.count()\n",
    "    return (nodes / edges)\n",
    "\n",
    "# SLUTT AV FUNKSJONER\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e41ec326-87f0-48a4-af19-38e543952f73",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Info\n",
    "Bellow you will see three different functions called Prototype 1-3. This is because i expiremented with different functions to see which one is the fastest. So therefore we will be using function 3 in the example, because it compiles the data the fastest. While 2 is slower and 1 is the slowest.\n",
    "You can also try this out with the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6febdeb5-3664-451d-aa6b-aebab4b861bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n|Reputation|           Diversity|\n+----------+--------------------+\n|     213.0|0.009404388714733543|\n|    2952.0|  1.0376175548589341|\n|     173.0|0.009404388714733543|\n|     325.0| 0.03134796238244514|\n|     176.0|0.009404388714733543|\n|     283.0|0.009404388714733543|\n|     403.0|0.047021943573667714|\n|     721.0|0.009404388714733543|\n|     121.0|   0.109717868338558|\n|     549.0| 0.31347962382445144|\n|    3967.0|  2.0595611285266457|\n|     971.0| 0.27586206896551724|\n|     355.0|0.003134796238244514|\n|     273.0|0.003134796238244514|\n|    4832.0|   3.310344827586207|\n|     123.0| 0.03134796238244514|\n|     151.0|0.009404388714733543|\n|     443.0|0.047021943573667714|\n|     634.0|0.003134796238244514|\n|    2182.0|   5.078369905956113|\n+----------+--------------------+\nonly showing top 20 rows\n\nComputed Pearsons Prototype3: 0.6877209756913499\n"
     ]
    }
   ],
   "source": [
    "# 3 PROTOTYPE FUNCTIONS\n",
    "# PROTOTYPE 3 is computing the fastest\n",
    "\n",
    "def Prototype1(df1: DataFrame, df2: DataFrame) -> DataFrame:\n",
    "    data = []\n",
    "    df1.createOrReplaceTempView(\"df1\")\n",
    "    df2.createOrReplaceTempView(\"df2\")\n",
    "    df1_filtered = spark.sql(f\"SELECT DISTINCT Id, CAST(Reputation AS float) FROM df1\")\n",
    "    df2_filtered = spark.sql(f\"SELECT OwnerUserId, Tags FROM df2 WHERE PostTypeId = 1\")\n",
    "    df1_filtered.createOrReplaceTempView(\"df1_filtered\")\n",
    "    df2_filtered.createOrReplaceTempView(\"df2_filtered\")\n",
    "    for row in df1_filtered.collect():\n",
    "        userId = row['Id']\n",
    "        Rep = row['Reputation']\n",
    "        # a = variableA(df1_filtered, userId)\n",
    "        b = variableB(df2_filtered, userId)\n",
    "        if (b != 0):\n",
    "            data.append((Rep, float(b)))\n",
    "            # Only do the first 5\n",
    "            if(len(data) > 5):\n",
    "              return spark.createDataFrame(data, ['Reputation','Diversity'])\n",
    "\n",
    "    return spark.createDataFrame(data, ['Reputation','Diversity'])\n",
    "\n",
    "def Prototype2(df1: DataFrame, df2: DataFrame) -> DataFrame:\n",
    "    data = []\n",
    "    df1.createOrReplaceTempView(\"df1\")\n",
    "    df2.createOrReplaceTempView(\"df2\")\n",
    "    df12 = spark.sql(f\"SELECT OwnerUserId, Reputation, collect_list(Tags) AS Tags FROM df1 JOIN df2 ON df1.Id = df2.OwnerUserId GROUP BY OwnerUserId, Reputation\")\n",
    "    df12.createOrReplaceTempView(\"df12\")\n",
    "    rows = df12.collect()\n",
    "    for row in rows:\n",
    "        userId = row['OwnerUserId']\n",
    "        Rep = row['Reputation']\n",
    "        # a = variableA(df1_filtered, userId)\n",
    "        b = variableB(df12, userId)\n",
    "        if (b != 0):\n",
    "            data.append((Rep, float(b)))\n",
    "            #Only do the first 5\n",
    "            if (len(data) > 5):\n",
    "              return spark.createDataFrame(data, ['Reputation','Diversity'])\n",
    "\n",
    "    return spark.createDataFrame(data, ['Reputation','Diversity'])\n",
    "  \n",
    "\n",
    "def Prototype3(df1: DataFrame, df2: DataFrame) -> DataFrame:\n",
    "    data = []\n",
    "    df1.createOrReplaceTempView(\"df1\")\n",
    "    df2.createOrReplaceTempView(\"df2\")\n",
    "    df12 = spark.sql(f\"SELECT OwnerUserId, CAST(Reputation AS float), concat_ws('', collect_list(Tags)) AS Tags FROM df1 JOIN df2 ON df1.Id = df2.OwnerUserId GROUP BY OwnerUserId, Reputation\")\n",
    "    df12.createOrReplaceTempView(\"df12\")\n",
    "    rows = df12.collect()\n",
    "    for row in rows:\n",
    "      userId = row['OwnerUserId']\n",
    "      Rep = row['Reputation']\n",
    "      tags = row['Tags']\n",
    "      if tags:\n",
    "        nodes = 638\n",
    "        edge = get_edges(make_tag_graph(spark.sql(f\"SELECT * FROM df12 WHERE OwnerUserId = {userId}\")))\n",
    "        if (edge is not None):\n",
    "          answer = edge.count() / nodes\n",
    "          data.append((Rep, answer))\n",
    "          # Only do the first 50 (Takes the same amount of time as 1 and 2 does to compute 5.)\n",
    "          # Therefore Prototype3 is almost 10 times faster than 1 and 2\n",
    "          if(len(data) > 50):\n",
    "              return spark.createDataFrame(data, ['Reputation','Diversity'])\n",
    "\n",
    "    return spark.createDataFrame(data, ['Reputation','Diversity'])\n",
    "  \n",
    "\n",
    "# pro1 = Prototype1(users_df,posts_df)\n",
    "# pro2 = Prototype2(users_df,posts_df)\n",
    "pro3 = Prototype3(users_df,posts_df)\n",
    "\n",
    "# pro1.show()\n",
    "# pro1pearson = compute_pearsons_r(pro1, \"Reputation\", \"Diversity\")\n",
    "# print(f\"Computed Pearsons Prototype1: {pro1pearson}\")\n",
    "\n",
    "# pro2.show()\n",
    "# pro2pearson = compute_pearsons_r(pro2, \"Reputation\", \"Diversity\")\n",
    "# print(f\"Computed Pearsons Prototype2: {pro2pearson}\")\n",
    "\n",
    "pro3.show()\n",
    "pro3pearson = compute_pearsons_r(pro3, \"Reputation\", \"Diversity\")\n",
    "print(f\"Computed Pearsons Prototype3: {pro3pearson}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35256328-cba3-40df-95ed-df77a57eea69",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Explanation\n",
    "The value of the pearson correlation coeeficient indicates that there is a strong positive correlation between users expertise level and their diversity interests. This means that the higher the users expertise level, the more likely they are to have a higher diversity for topics aswell.\n",
    "\n",
    "It is also considered a very strong correlation. We can also interpret the answer as the expert users are likely to have general interests rather then specific ones, that can show how they have gotten such a high expertise level"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Task4",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
